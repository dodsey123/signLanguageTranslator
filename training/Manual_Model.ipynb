{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50125c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses VTT files and extracts the start time, end time, and text.\n",
    "def parse_vtt(vtt_path, video_id):\n",
    "    # Function to convert time string to seconds\n",
    "    def time_str_to_seconds(time_str):\n",
    "        h, m, s = time_str.split(\":\")\n",
    "        s, ms = s.split(\".\")\n",
    "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "    # Read the VTT file\n",
    "    with open(vtt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vtt_text = f.read()\n",
    "\n",
    "    # Remove WebVTT header \n",
    "    blocks = re.split(r'\\n\\n+', vtt_text.strip())\n",
    "    # Create a list to store entries\n",
    "    entries = []\n",
    "\n",
    "    # Iterate through each block and extract the start time, end time, and text\n",
    "    for block in blocks:\n",
    "        # Strip whitespace and split by lines\n",
    "        lines = block.strip().splitlines()\n",
    "        # If the block has at least 2 lines and the first line contains \"-->\", it's a valid entry\n",
    "        if len(lines) >= 2 and \"-->\" in lines[0]:\n",
    "            # Extract start time, end time, text and video_id to distinguish overlapping times\n",
    "            start, end = lines[0].split(\" --> \")\n",
    "            text = \" \".join(lines[1:]).strip()\n",
    "            entries.append({\n",
    "                \"start\": time_str_to_seconds(start.strip()),\n",
    "                \"end\": time_str_to_seconds(end.strip()),\n",
    "                \"text\": text,\n",
    "                \"video_id\": video_id \n",
    "            })\n",
    "\n",
    "    # Return the list of entries\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts the video ID from the file path.\n",
    "def extract_video_id(path):\n",
    "    return os.path.basename(path).split(\"_keypoints\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19409a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the keypoints and subtitles files and create a list of files\n",
    "keypoints_path = \"<keypoints_path_here>\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"<subtitles_path_here>\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "# Create a list to store keypoints and subtitles\n",
    "keypoints = []\n",
    "subtitles = []\n",
    "\n",
    "# Iterate through the keypoints files and load them\n",
    "for k in keypoints_files:\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  print(k)\n",
    "  temp_keypoints = torch.load(k)\n",
    "  # Append the video ID and keypoints to the list\n",
    "  keypoints.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "counter  = 0\n",
    "\n",
    "# Iterate through the subtitles files and parse them\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    # Append the video ID and parsed subtitles to the list\n",
    "    subtitles.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the keypoints and subtitles files and create a list of files\n",
    "keypoints_path = \"<keypoints_path_here>\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"<subtitles_path_here>\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "# Create a list to store test keypoints and subtitles\n",
    "keypoints_test = []\n",
    "subtitles_test = []\n",
    "\n",
    "# Iterate through the keypoints files and load them\n",
    "for k in keypoints_files:\n",
    "  print(k)\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  temp_keypoints = torch.load(k)\n",
    "  # Append the video ID and keypoints to the list\n",
    "  keypoints_test.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "# Iterate through the subtitles files and parse them\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    # Append the video ID and parsed subtitles to the list\n",
    "    subtitles_test.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert subtitle times to seconds\n",
    "def time_to_float(time_str):\n",
    "\n",
    "    # Split the time string into hours, minutes, and seconds\n",
    "    hours, minutes, seconds = time_str.split(':')\n",
    "\n",
    "    # Convert to float\n",
    "    hours = float(hours)\n",
    "    minutes = float(minutes)\n",
    "    seconds = float(seconds)\n",
    "\n",
    "    # Convert to total seconds\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    # Return the total seconds as a float\n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528525b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for keypoints and subtitles\n",
    "class ManualKeypointsAndSubtitlesDataset(Dataset):\n",
    "    def __init__(self, keypoints_data, subtitle_entries, fps=25, tokenizer=None, max_length=80, num_joints=25):\n",
    "        # Set the fps\n",
    "        self.fps = fps\n",
    "        # Set the max length for tokenization\n",
    "        self.max_length = max_length\n",
    "        # Set the number of joints for keypoints\n",
    "        self.num_joints = num_joints\n",
    "        # Set the device for PyTorch\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Set the tokenizer for text processing\n",
    "        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        # Create a dictionary for keypoints and subtitles sample data\n",
    "        self.samples = self.build_samples(keypoints_data, subtitle_entries)\n",
    "\n",
    "    # Build samples from keypoints, subtitles, and emotions\n",
    "    def build_samples(self, keypoints_dict, subtitles):\n",
    "        # Create a dictionary for samples\n",
    "        samples = []\n",
    "\n",
    "        # Iterate through the subtitles and extract keypoints\n",
    "        for sub in subtitles:\n",
    "            # Extract video ID, start time, and end time\n",
    "            video_id = sub[\"video_id\"]\n",
    "            start_frame = int(sub[\"start\"] * self.fps)\n",
    "            end_frame = int(sub[\"end\"] * self.fps)\n",
    "\n",
    "            # Check if the video ID exists in keypoints data\n",
    "            if video_id not in keypoints_dict:\n",
    "                continue\n",
    "\n",
    "            # Extract keypoints for the video ID\n",
    "            video_kps = keypoints_dict[video_id]\n",
    "\n",
    "            # Check if the start and end frames are within the bounds of the keypoints\n",
    "            if end_frame > len(video_kps):\n",
    "                continue\n",
    "\n",
    "            # Extract the keypoints and emotions for the specified time range\n",
    "            keypoints_seq = video_kps[start_frame:end_frame]\n",
    "\n",
    "            # Check if the sequences are empty\n",
    "            if len(keypoints_seq) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process keypoints\n",
    "            processed_kps = []\n",
    "            # Iterate through the keypoints \n",
    "            for frame in keypoints_seq:\n",
    "                # Flatten the keypoints and pad with zeros if necessary\n",
    "                frame_tensor = torch.zeros(self.num_joints * 3)\n",
    "                # Check if the frame is empty \n",
    "                if len(frame) > 0:\n",
    "                    # Get the first person in the frame\n",
    "                    person = frame[0]\n",
    "                    # Flatten the keypoints and pad with zeros if necessary\n",
    "                    flat_kps = [coord for part in person for joint in part for coord in joint]\n",
    "                    flat_kps = flat_kps[:self.num_joints * 3] + [0] * max(0, self.num_joints * 3 - len(flat_kps))\n",
    "                    # Convert to tensor\n",
    "                    frame_tensor = torch.tensor(flat_kps, dtype=torch.float32)\n",
    "                # Append the tensor to the list\n",
    "                processed_kps.append(frame_tensor)\n",
    "\n",
    "            # Pad the sequences to the maximum length\n",
    "            text = sub[\"text\"]\n",
    "            tokenized = self.tokenizer(text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "            # Append the sample to the list\n",
    "            samples.append((torch.stack(processed_kps), tokenized))\n",
    "\n",
    "        # Return the list of samples\n",
    "        return samples\n",
    "\n",
    "    # Get the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # Get a sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2322008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer for text processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create the dictionary for keypoints\n",
    "keypoints_dict = dict(keypoints) \n",
    "\n",
    "# Create the dataset for training\n",
    "manual_train_dataset = ManualKeypointsAndSubtitlesDataset(keypoints_dict, subtitles, fps=25, tokenizer=tokenizer)\n",
    "\n",
    "# Print the dataset size and the shape of the first sample\n",
    "print(\"Dataset size:\", len(manual_train_dataset))\n",
    "k, s = manual_train_dataset[0]\n",
    "print(\"Keypoints Shape:\", k.shape) \n",
    "print(\"Subtitles Shape:\", s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57506960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer for text processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create the dictionary for test keypoints \n",
    "keypoints_dict_test = dict(keypoints_test)\n",
    "\n",
    "# Create the dataset for testing\n",
    "manual_test_dataset = ManualKeypointsAndSubtitlesDataset(keypoints_dict_test, subtitles_test, fps=25, tokenizer=tokenizer)\n",
    "\n",
    "# Print the dataset size and the shape of the first sample\n",
    "print(\"Dataset size:\", len(manual_test_dataset))\n",
    "k, s = manual_test_dataset[0]\n",
    "print(\"Keypoints Shape:\", k.shape) \n",
    "print(\"Subtitles Shape:\", s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b744cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collate and pad the samples into batches\n",
    "def manual_collate_fn(batch):\n",
    "    k, s = zip(*batch)\n",
    "\n",
    "    # Pad the keypoints, emotions, and subtitles sequences\n",
    "    keypoints_padded = pad_sequence(k, batch_first=True, padding_value=0.0)  # [B, T, 75]\n",
    "    subtitles_padded = pad_sequence(s, batch_first=True, padding_value=0)    # [B, L]\n",
    "\n",
    "    # Return the padded sequences\n",
    "    return keypoints_padded, subtitles_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for training\n",
    "manual_train_loader = DataLoader(manual_train_dataset, batch_size=128, shuffle=True, collate_fn=manual_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd898eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for testing\n",
    "manual_test_loader = DataLoader(manual_test_dataset, batch_size=128, shuffle=True, collate_fn=manual_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082701a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Positional Encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Create a positional encoding matrix\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the positional encoding using sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        # Check if the input tensor exceeds the maximum length\n",
    "        if x.size(1) > self.pe.size(1):\n",
    "            raise ValueError(f\"Sequence length {x.size(1)} exceeds max positional encoding length {self.pe.size(1)}\")\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder-Decoder Transformer model for manual keypoints and subtitles\n",
    "class ManualEncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, keypoints_dim=75, d_model=384, num_heads=6, num_layers=4, ff_dim=512, max_len=1024, vocab_size=30522, pad_idx=0):\n",
    "        super().__init__()\n",
    "        # Keypoints projection layer to transform keypoints to the model dimension\n",
    "        self.keypoints_proj = nn.Linear(keypoints_dim, d_model)\n",
    "        # Dropout layer for regularization\n",
    "        self.input_dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Positional encoding layers for encoder and decoder\n",
    "        self.encoder_pe = PositionalEncoding(d_model, max_len)\n",
    "        self.decoder_pe = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, ff_dim, dropout=0.1)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Transformer decoder layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, ff_dim, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        # Embedding layer for text input\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.output_fc = nn.Linear(d_model, vocab_size)\n",
    "        # Weights tying for the output layer\n",
    "        self.output_fc.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, keypoints, input_ids, tgt_mask=None):\n",
    "        # Mask for the source sequence (keypoints)\n",
    "        src_mask = keypoints.abs().sum(dim=-1) == 0\n",
    "        # Mask for the target sequence (subtitles)\n",
    "        tgt_pad_mask = input_ids == 0\n",
    "\n",
    "        # Apply the keypoints projection and positional encoding\n",
    "        x = self.keypoints_proj(keypoints)\n",
    "        x = self.encoder_pe(self.input_dropout(x)).permute(1, 0, 2)\n",
    "        memory = self.encoder(x, src_key_padding_mask=src_mask)\n",
    "\n",
    "        # Apply the embedding and positional encoding for the target sequence\n",
    "        tgt = self.embedding(input_ids)\n",
    "        tgt = self.decoder_pe(self.input_dropout(tgt)).permute(1, 0, 2)\n",
    "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=src_mask)\n",
    "\n",
    "        # Apply the output fully connected layer to get the final output\n",
    "        return self.output_fc(out.permute(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the device\n",
    "# Only run this when this is the first run\n",
    "manual_model = ManualEncoderDecoderTransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19453dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this when loading an existing model\n",
    "# Recreate the model and load the weights\n",
    "manual_model = ManualEncoderDecoderTransformer().to(device)\n",
    "\n",
    "# Load the model weights\n",
    "manual_model.load_state_dict(torch.load(\"<path_to_model>\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38caaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding function\n",
    "def sample_decode_beam(model, keypoints, tokenizer, beam_width=3, max_len=300, eos_token_id=102):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Define the device\n",
    "    device = keypoints.device\n",
    "    # Get generated sequences\n",
    "    generated = [(torch.tensor([tokenizer.cls_token_id], device=device), 0.0)]\n",
    "\n",
    "    # Iterate for the maximum length of the sequence\n",
    "    for _ in range(max_len):\n",
    "        # Create a list to store all candidates\n",
    "        all_candidates = []\n",
    "        # Iterate through the generated sequences\n",
    "        for seq, score in generated:\n",
    "            # Check if the last token is the end-of-sequence token\n",
    "            if seq[-1].item() == eos_token_id:\n",
    "                # If sp add it to the candidates\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Unsqueeze the sequence to add batch dimension\n",
    "            input_ids = seq.unsqueeze(0)\n",
    "            # Generate the target mask\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(input_ids.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass through the model\n",
    "                logits = model(keypoints.unsqueeze(0), input_ids, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the logits for the last token\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            # Get the top k tokens and their probabilities\n",
    "            top_probs, top_indices = probs.topk(beam_width)\n",
    "\n",
    "            # Iterate through the beam width\n",
    "            for i in range(beam_width):\n",
    "                # Create a candidate sequence\n",
    "                candidate = torch.cat([seq, top_indices[i].unsqueeze(0)])\n",
    "                # Append the candidate and its score to the list\n",
    "                all_candidates.append((candidate, score - torch.log(top_probs[i] + 1e-12)))\n",
    "\n",
    "        # Sort the candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1])\n",
    "        # Keep only the top k candidates\n",
    "        generated = ordered[:beam_width]\n",
    "\n",
    "    # Get the best candidate\n",
    "    return tokenizer.decode(generated[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and validate the model\n",
    "def train_validate_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "    device, tokenizer, num_epochs=300, eos_token_id=102, pad_token_id=0,\n",
    "):\n",
    "    # Initialize the values used in training\n",
    "    best_val_loss = float('inf')\n",
    "    patience, patience_counter = 15, 0\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Iterate through the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        # Initialize the values for training\n",
    "        total_train_loss, total_train_correct, total_train_tokens = 0, 0, 0\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for keypoints, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "          # Skip long sequences\n",
    "          if keypoints.size(1) > 1024 or targets.size(1) > 80:\n",
    "            print(\"Skipping long sequence\")\n",
    "            continue\n",
    "          else:\n",
    "            # Move the keypoints and targets to the device\n",
    "            keypoints, targets = keypoints.to(device), targets.to(device)\n",
    "\n",
    "            # Create the decoder input and target sequences\n",
    "            decoder_input = targets[:, :-1]\n",
    "            decoder_target = targets[:, 1:]\n",
    "            # Create the target mask\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through the model\n",
    "            logits = model(keypoints, decoder_input, tgt_mask=tgt_mask)\n",
    "            # Compute the loss\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), decoder_target.reshape(-1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = (decoder_target != pad_token_id) & (decoder_target != eos_token_id)\n",
    "            correct = ((preds == decoder_target) & mask).sum().item()\n",
    "            total = mask.sum().item()\n",
    "\n",
    "            # Update the total values\n",
    "            total_train_correct += correct\n",
    "            total_train_tokens += total\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Compute the average accuracy and loss for training\n",
    "        train_acc = total_train_correct / total_train_tokens * 100\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Append the training accuracy and loss to the lists\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Initialize the values for validation\n",
    "        total_val_loss, total_val_correct, total_val_tokens = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            # Iterate through the validation data\n",
    "            for keypoints, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "              # Skip long sequences\n",
    "              if keypoints.size(1) > 1024 or targets.size(1) > 80:\n",
    "                print(\"Skipping long sequence\")\n",
    "                continue\n",
    "              else:\n",
    "                # Move the keypoints and targets to the device\n",
    "                keypoints, targets = keypoints.to(device), targets.to(device)\n",
    "\n",
    "                # Create the decoder input and target sequences\n",
    "                decoder_input = targets[:, :-1]\n",
    "                decoder_target = targets[:, 1:]\n",
    "                # Create the target mask\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                logits = model(keypoints, decoder_input, tgt_mask=tgt_mask)\n",
    "                # Compute the loss\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), decoder_target.reshape(-1))\n",
    "\n",
    "                # Compute the accuracy\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                mask = decoder_target != pad_token_id\n",
    "                correct = ((preds == decoder_target) & mask).sum().item()\n",
    "                total = mask.sum().item()\n",
    "\n",
    "                # Update the total values\n",
    "                total_val_correct += correct\n",
    "                total_val_tokens += total\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        # Compute the average accuracy and loss for validation\n",
    "        val_acc = total_val_correct / total_val_tokens * 100\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # Append the validation accuracy and loss to the lists\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print the summary for the epoch\n",
    "        print(f\"Epoch {epoch+1} Summary:\\nTrain Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Create sample\n",
    "        keypoints_sample, _ = next(iter(val_loader))\n",
    "\n",
    "        for keypoints_sample, targets_sample in val_loader:\n",
    "          if keypoints_sample.size(1) <= 1024:\n",
    "            sample_text = sample_decode_beam(model, keypoints_sample[0].to(device), tokenizer)\n",
    "            # Print the sample text\n",
    "            print(\"Sample decoded:\", sample_text)\n",
    "            break\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            # Save the best model\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_manual_model_batch1.pt\")\n",
    "        else:\n",
    "            # Increment the patience counter\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. {patience_counter} / {patience}\")\n",
    "            # Check if patience is exceeded\n",
    "            if patience_counter >= patience:\n",
    "                # Stop training\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Plotting\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy Over Epochs\"), plt.legend(), plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Over Epochs\"), plt.legend(), plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84927a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion for the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "# Define the optimizer for the model\n",
    "optimizer = torch.optim.AdamW(manual_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(manual_train_loader) * 500 # num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d64f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache and collect garbage\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate the model\n",
    "train_validate_model(manual_model, manual_train_loader, manual_test_loader, optimizer, criterion, scheduler, device, tokenizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59874f15",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "Epoch 1 Summary:\n",
    "Train Loss: 82.1442, Acc: 0.35% | Val Loss: 71.8930, Acc: 0.66%\n",
    "Sample decoded: buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons buttons\n",
    "Epoch 2/500 [Train]: 100%|██████████| 39/39 [00:09<00:00,  4.11it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 12.12it/s]\n",
    "Epoch 2 Summary:\n",
    "Train Loss: 60.1969, Acc: 0.82% | Val Loss: 48.1628, Acc: 2.61%\n",
    "Sample decoded: bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian bolivian\n",
    "Epoch 3/500 [Train]: 100%|██████████| 39/39 [00:09<00:00,  3.95it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 12.01it/s]\n",
    "Epoch 3 Summary:\n",
    "Train Loss: 43.3728, Acc: 1.14% | Val Loss: 35.9727, Acc: 2.90%\n",
    "Sample decoded: ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
    "Epoch 4/500 [Train]: 100%|██████████| 39/39 [00:09<00:00,  4.06it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 12.42it/s]\n",
    "Epoch 4 Summary:\n",
    "Train Loss: 35.4826, Acc: 1.43% | Val Loss: 30.8406, Acc: 3.98%\n",
    "Sample decoded: ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
    "Epoch 5/500 [Train]: 100%|██████████| 39/39 [00:09<00:00,  4.03it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 12.09it/s]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
