{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import decimal\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f039af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses VTT files and extracts the start time, end time, and text.\n",
    "def parse_vtt(vtt_path, video_id):\n",
    "    # Function to convert time string to seconds\n",
    "    def time_str_to_seconds(time_str):\n",
    "        h, m, s = time_str.split(\":\")\n",
    "        s, ms = s.split(\".\")\n",
    "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "    # Read the VTT file\n",
    "    with open(vtt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vtt_text = f.read()\n",
    "\n",
    "    # Remove WebVTT header \n",
    "    blocks = re.split(r'\\n\\n+', vtt_text.strip())\n",
    "    # Create a list to store entries\n",
    "    entries = []\n",
    "\n",
    "    # Iterate through each block and extract the start time, end time, and text\n",
    "    for block in blocks:\n",
    "        # Strip whitespace and split by lines\n",
    "        lines = block.strip().splitlines()\n",
    "        # If the block has at least 2 lines and the first line contains \"-->\", it's a valid entry\n",
    "        if len(lines) >= 2 and \"-->\" in lines[0]:\n",
    "            # Extract start time, end time, text and video_id to distinguish overlapping times\n",
    "            start, end = lines[0].split(\" --> \")\n",
    "            text = \" \".join(lines[1:]).strip()\n",
    "            entries.append({\n",
    "                \"start\": time_str_to_seconds(start.strip()),\n",
    "                \"end\": time_str_to_seconds(end.strip()),\n",
    "                \"text\": text,\n",
    "                \"video_id\": video_id \n",
    "            })\n",
    "\n",
    "    # Return the list of entries\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts the video ID from the file path.\n",
    "def extract_video_id(path):\n",
    "    return os.path.basename(path).split(\"_keypoints\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181549a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the keypoints and subtitles files and create a list of files\n",
    "keypoints_path = \"<keypoints_path_here>\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"<subtitles_path_here>\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "# Create a list to store keypoints and subtitles\n",
    "keypoints = []\n",
    "subtitles = []\n",
    "\n",
    "# Iterate through the keypoints files and load them\n",
    "for k in keypoints_files:\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  print(k)\n",
    "  temp_keypoints = torch.load(k)\n",
    "  # Append the video ID and keypoints to the list\n",
    "  keypoints.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "counter  = 0\n",
    "\n",
    "# Iterate through the subtitles files and parse them\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    # Append the video ID and parsed subtitles to the list\n",
    "    subtitles.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a046b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load emotion tensor from JSON file\n",
    "def load_emotion_tensor_from_json(json_path, video_id):\n",
    "    # Read the JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(len(data))\n",
    "\n",
    "    # Create a list to store the normalized tensors\n",
    "    normalized = []\n",
    "\n",
    "    # Iterate through the data \n",
    "    for i, frame in enumerate(data):\n",
    "        # Check if the frame is empty or all zeros\n",
    "        if not frame or all(v == 0 for v in frame):\n",
    "            # Padd with zeros for no emotion data or all-zero frame\n",
    "            normalized.append(torch.zeros(7, dtype=torch.float32))\n",
    "        else:\n",
    "            # Convert using Decimal for precision, but store as float32 tensor\n",
    "            values = [float(decimal.Decimal(str(x))) for x in frame]\n",
    "            tensor = torch.tensor(values, dtype=torch.float32)\n",
    "            normalized.append(tensor)\n",
    "\n",
    "    return (video_id, torch.stack(normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the precision for printing tensors\n",
    "torch.set_printoptions(precision=10, sci_mode=True)\n",
    "\n",
    "# Path to the emotion tensors and create a list of files\n",
    "emotions_path = \"<emotions_path_here>\"\n",
    "emotions_files = glob(os.path.join(emotions_path, \"*\"))\n",
    "\n",
    "print(len(emotions_files))\n",
    "\n",
    "# Create a list to store emotion tensors\n",
    "emotions = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Iterate through the emotion files and load them\n",
    "for e in emotions_files:\n",
    "  print(e)\n",
    "  base_name = os.path.basename(e).replace(\".json\", \"\")\n",
    "  video_id, emotion_tensor = load_emotion_tensor_from_json(e, base_name)\n",
    "  # Append the video ID and emotion tensor to the list\n",
    "  emotions.append((video_id, emotion_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbe0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the keypoints and subtitles files and create a list of files\n",
    "keypoints_path = \"<keypoints_path_here>\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"<subtitles_path_here>\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "# Create a list to store test keypoints and subtitles\n",
    "keypoints_test = []\n",
    "subtitles_test = []\n",
    "\n",
    "# Iterate through the keypoints files and load them\n",
    "for k in keypoints_files:\n",
    "  print(k)\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  temp_keypoints = torch.load(k)\n",
    "  # Append the video ID and keypoints to the list\n",
    "  keypoints_test.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "# Iterate through the subtitles files and parse them\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    # Append the video ID and parsed subtitles to the list\n",
    "    subtitles_test.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6305d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the emotion tensors and create a list of files\n",
    "emotions_path = \"<emotions_path_here>\"\n",
    "emotions_files = glob(os.path.join(emotions_path, \"*\"))\n",
    "\n",
    "print(len(emotions_files))\n",
    "\n",
    "# Create a list to store test emotion tensors\n",
    "emotions_test = []\n",
    "\n",
    "# Iterate through the emotion files and load them\n",
    "for e in emotions_files:\n",
    "    print(e)\n",
    "    base_name = os.path.basename(e).replace(\".json\", \"\")\n",
    "    tensor = load_emotion_tensor_from_json(e, base_name)\n",
    "    # Append the video ID and emotion tensor to the list\n",
    "    emotions_test.append(load_emotion_tensor_from_json(e, base_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert subtitle times to seconds\n",
    "def time_to_float(time_str):\n",
    "\n",
    "    # Split the time string into hours, minutes, and seconds\n",
    "    hours, minutes, seconds = time_str.split(':')\n",
    "\n",
    "    # Convert to float\n",
    "    hours = float(hours)\n",
    "    minutes = float(minutes)\n",
    "    seconds = float(seconds)\n",
    "\n",
    "    # Convert to total seconds\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    # Return the total seconds as a float\n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e085ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for keypoints, emotions and subtitles\n",
    "class EmotionKeypointsAndSubtitlesDataset(Dataset):\n",
    "    def __init__(self, keypoints_data, subtitle_entries, emotion_data, fps=25, tokenizer=None, max_length=80, num_joints=25):\n",
    "        # Set the fps\n",
    "        self.fps = fps\n",
    "        # Set the maximum length for tokenization\n",
    "        self.max_length = max_length\n",
    "        # Set the number of joints for keypoints\n",
    "        self.num_joints = num_joints\n",
    "        # Set the tokenizer for text processing\n",
    "        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        # Create a dictionary for keypoints, emotion and subtitles sample data\n",
    "        self.samples = self.build_samples(keypoints_data, subtitle_entries, emotion_data)\n",
    "\n",
    "    # Build samples from keypoints, subtitles, and emotions\n",
    "    def build_samples(self, keypoints_data, subtitles, emotion_data):\n",
    "        # Create a dictionary for samples\n",
    "        samples = []\n",
    "\n",
    "        # Iterate through the subtitles and extract keypoints and emotions\n",
    "        for sub in subtitles:\n",
    "            # Extract video ID, start time, and end time\n",
    "            video_id = sub[\"video_id\"]\n",
    "            start_frame = int(sub[\"start\"] * self.fps)\n",
    "            end_frame = int(sub[\"end\"] * self.fps)\n",
    "\n",
    "            # Check if the video ID exists in keypoints and emotion data\n",
    "            if video_id not in keypoints_data or video_id not in emotion_data:\n",
    "                continue\n",
    "\n",
    "            # Extract keypoints and emotions for the video ID\n",
    "            video_kps = keypoints_data[video_id]\n",
    "            video_emotions = emotion_data[video_id]\n",
    "\n",
    "            # Check if the start and end frames are within the bounds of the keypoints and emotions\n",
    "            if end_frame > len(video_kps) or end_frame > len(video_emotions):\n",
    "                continue\n",
    "\n",
    "            # Extract the keypoints and emotions for the specified time range\n",
    "            keypoints_seq = video_kps[start_frame:end_frame]\n",
    "            emotion_seq = video_emotions[start_frame:end_frame]\n",
    "\n",
    "            # Check if the sequences are empty\n",
    "            if len(keypoints_seq) == 0 or len(emotion_seq) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process keypoints\n",
    "            processed_kps = []\n",
    "            # Iterate through the keypoints \n",
    "            for frame in keypoints_seq:\n",
    "                # Flatten the keypoints and pad with zeros if necessary\n",
    "                frame_tensor = torch.zeros(self.num_joints * 3)\n",
    "                # Check if the frame is empty \n",
    "                if len(frame) > 0:\n",
    "                    # Get the first person in the frame\n",
    "                    person = frame[0]\n",
    "                    # Flatten the keypoints and pad with zeros if necessary\n",
    "                    flat_kps = [coord for part in person for joint in part for coord in joint]\n",
    "                    flat_kps = flat_kps[:self.num_joints * 3] + [0] * max(0, self.num_joints * 3 - len(flat_kps))\n",
    "                    # Convert to tensor\n",
    "                    frame_tensor = torch.tensor(flat_kps, dtype=torch.float32)\n",
    "                # Append the tensor to the list\n",
    "                processed_kps.append(frame_tensor)\n",
    "\n",
    "            # Process emotions\n",
    "            processed_emo = []\n",
    "            # Iterate through the emotions\n",
    "            for emo in emotion_seq:\n",
    "                # Convert to tensor\n",
    "                emo_tensor = torch.tensor(emo, dtype=torch.float32)\n",
    "                # Append the tensor to the list\n",
    "                processed_emo.append(emo_tensor)\n",
    "\n",
    "            # Check if the lengths of keypoints and emotions match\n",
    "            if len(processed_emo) != len(processed_kps):\n",
    "                continue\n",
    "\n",
    "            # Pad the sequences to the maximum length\n",
    "            text = sub[\"text\"]\n",
    "            tokenized = self.tokenizer(text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "\n",
    "            # Append the sample to the list\n",
    "            samples.append((torch.stack(processed_kps), torch.stack(processed_emo), tokenized))\n",
    "\n",
    "        # Return the list of samples\n",
    "        return samples\n",
    "\n",
    "    # Get the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # Get a sample by index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ae743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer for text processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create the dictionary for keypoints and emotions\n",
    "keypoints_dict = dict(keypoints) \n",
    "emotions_dict = dict(emotions) \n",
    "\n",
    "# Create the dataset for training\n",
    "emotion_train_dataset = EmotionKeypointsAndSubtitlesDataset(keypoints_dict, subtitles, emotions_dict, fps=25, tokenizer=tokenizer)\n",
    "\n",
    "# Print the number of samples in the dataset\n",
    "print(\"Dataset size:\", len(emotion_train_dataset))\n",
    "\n",
    "# Print the shape of the first sample in the dataset\n",
    "k, e, s = emotion_train_dataset[0]\n",
    "print(\"Keypoints Shape:\", k.shape) \n",
    "print(\"Subtitles Shape:\", s.shape)\n",
    "print(\"Emotions Shape: \", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75566489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary for keypoints and emotions for testing\n",
    "keypoints_test_dict = dict(keypoints_test) \n",
    "emotions_test_dict = dict(emotions_test)\n",
    "\n",
    "# Create the dataset for testing\n",
    "emotion_test_dataset = EmotionKeypointsAndSubtitlesDataset(keypoints_test_dict, subtitles_test, emotions_test_dict, fps=25, tokenizer=tokenizer)\n",
    "\n",
    "# Print the number of samples in the test dataset\n",
    "print(\"Dataset size:\", len(emotion_test_dataset))\n",
    "\n",
    "# Print the shape of the first sample in the test dataset\n",
    "k, s, e = emotion_test_dataset[0]\n",
    "print(\"Keypoints Shape:\", k.shape) \n",
    "print(\"Subtitles Shape:\", s.shape)\n",
    "print(\"Emotions Shape: \", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collate and pad the samples into batches\n",
    "def emotion_collate_fn(batch):\n",
    "    k, e, s = zip(*batch)\n",
    "\n",
    "    # Pad the keypoints, emotions, and subtitles sequences\n",
    "    keypoints_padded = pad_sequence(k, batch_first=True, padding_value=0.0)  # [B, T, 75]\n",
    "    emotions_padded = pad_sequence(e, batch_first=True, padding_value=0.0)    # [B, T, 7]\n",
    "    subtitles_padded = pad_sequence(s, batch_first=True, padding_value=0)    # [B, L]\n",
    "\n",
    "    # Return the padded sequences\n",
    "    return keypoints_padded, emotions_padded, subtitles_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for training\n",
    "emotion_train_loader = DataLoader(emotion_train_dataset, batch_size=128, shuffle=True, collate_fn=emotion_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for testing\n",
    "emotions_test_loader = DataLoader(emotion_test_dataset, batch_size=128, shuffle=True, collate_fn=emotion_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EmotionEncoderDecoderTransformer model\n",
    "class EmotionEncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_joints=25,\n",
    "                 kp_input_dim=3,\n",
    "                 emo_input_dim=7,\n",
    "                 hidden_size=256,\n",
    "                 num_layers=4,\n",
    "                 nhead=8,\n",
    "                 ff_dim=512,\n",
    "                 dropout=0.1,\n",
    "                 max_len=1024,\n",
    "                 vocab_size=30522):\n",
    "        super().__init__()\n",
    "\n",
    "        # Keypoints encoder\n",
    "        self.kp_fc = nn.Linear(num_joints * kp_input_dim, hidden_size)\n",
    "\n",
    "        # Emotions encoder\n",
    "        self.emotion_fc = nn.Linear(emo_input_dim, hidden_size)\n",
    "\n",
    "        # Emotion bias modulator for decoder\n",
    "        self.emo_to_bias = nn.Linear(emo_input_dim, hidden_size)\n",
    "\n",
    "        # Positional encoding buffer\n",
    "        pe = self._build_positional_encoding(max_len, hidden_size)\n",
    "        self.register_buffer(\"pos_encoder\", pe)  # [max_len, hidden_size]\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead,\n",
    "                                                   dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=nhead,\n",
    "                                                   dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    # Build the positional encoding\n",
    "    def _build_positional_encoding(self, max_len, hidden_size):\n",
    "        # Create a positional encoding matrix\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # Compute the positional encoding using sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(torch.log(torch.tensor(10000.0)) / hidden_size))\n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Return the positional encoding matrix\n",
    "        return pe \n",
    "\n",
    "    def forward(self, keypoints, emotions, tgt, tgt_mask=None):\n",
    "        device = keypoints.device\n",
    "\n",
    "        # Project keypoints and emotion\n",
    "        kp_feat = self.kp_fc(keypoints)           # (B, T, H)\n",
    "        emo_feat = self.emotion_fc(emotions)      # (B, T, H)\n",
    "\n",
    "        # Encoder input = keypoints + emotion bias + positional encoding\n",
    "        encoder_input = kp_feat + emo_feat\n",
    "        pos_enc = self.pos_encoder[:encoder_input.size(1)].to(device)  # (T, H)\n",
    "        encoder_input = encoder_input + pos_enc.unsqueeze(0)\n",
    "\n",
    "        # Encode\n",
    "        memory = self.encoder(encoder_input)\n",
    "\n",
    "        # Token embeddings for decoder\n",
    "        tgt_embed = self.token_embedding(tgt).to(device)\n",
    "        tgt_pos_enc = self.pos_encoder[:tgt.size(1)].to(device)\n",
    "        tgt_input = tgt_embed + tgt_pos_enc.unsqueeze(0)\n",
    "\n",
    "        # Apply emotion bias to decoder input (mean emotion across T)\n",
    "        emotion_bias = self.emo_to_bias(emotions.mean(dim=1))  # (B, H)\n",
    "        emotion_bias = emotion_bias.unsqueeze(1).expand(-1, tgt_input.size(1), -1)  # (B, T', H)\n",
    "        tgt_input = tgt_input + emotion_bias\n",
    "\n",
    "        # Decode\n",
    "        output = self.decoder(tgt_input, memory, tgt_mask=tgt_mask)\n",
    "        logits = self.output_fc(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the device\n",
    "# Only run this when this is the first run\n",
    "emotion_model = EmotionEncoderDecoderTransformer().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this when loading an existing model\n",
    "# Recreate the model and load the weights\n",
    "emotion_model = EmotionEncoderDecoderTransformer().to(device)\n",
    "\n",
    "# Load the model weights\n",
    "emotion_model.load_state_dict(torch.load(\"<path_to_model>\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding function\n",
    "def sample_decode_beam(model, keypoints, emotions, tokenizer, beam_width=3, max_len=300, eos_token_id=102):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Define the device\n",
    "    device = keypoints.device\n",
    "    # Get generated sequences\n",
    "    generated = [(torch.tensor([tokenizer.cls_token_id], device=device), 0.0)]\n",
    "\n",
    "    # Iterate for the maximum length of the sequence\n",
    "    for _ in range(max_len):\n",
    "        # Create a list to store all candidates\n",
    "        all_candidates = []\n",
    "        # Iterate through the generated sequences\n",
    "        for seq, score in generated:\n",
    "            # Check if the last token is the end-of-sequence token\n",
    "            if seq[-1].item() == eos_token_id:\n",
    "                # If sp add it to the candidates\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Unsqueeze the sequence to add batch dimension\n",
    "            input_ids = seq.unsqueeze(0)\n",
    "            # Generate the target mask\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(input_ids.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass through the model\n",
    "                logits = model(keypoints.unsqueeze(0), emotions.unsqueeze(0), input_ids, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Get the logits for the last token\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            # Get the top k tokens and their probabilities\n",
    "            top_probs, top_indices = probs.topk(beam_width)\n",
    "\n",
    "            # Iterate through the beam width\n",
    "            for i in range(beam_width):\n",
    "                # Create a candidate sequence\n",
    "                candidate = torch.cat([seq, top_indices[i].unsqueeze(0)])\n",
    "                # Append the candidate and its score to the list\n",
    "                all_candidates.append((candidate, score - torch.log(top_probs[i] + 1e-12)))\n",
    "\n",
    "        # Sort the candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1])\n",
    "        # Keep only the top k candidates\n",
    "        generated = ordered[:beam_width]\n",
    "\n",
    "    # Get the best candidate\n",
    "    return tokenizer.decode(generated[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5903c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model_emotion(model, train_loader, val_loader, optimizer, criterion, scheduler, device, tokenizer, num_epochs=500, eos_token_id=102, pad_token_id=0):\n",
    "    # Initialize the values used in training\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    scaler = GradScaler()\n",
    "    # Set patience for early stopping\n",
    "    patience, patience_counter = 15, 0\n",
    "\n",
    "    # Iterate through the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        # Initialize the values for training\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        # Define the scaler for mixed precision training\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for keypoints, emotions, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            # Move the data to the device\n",
    "            keypoints, emotions, targets = keypoints.to(device), emotions.to(device), targets.to(device)\n",
    "            # Create the decoder input and target sequences\n",
    "            decoder_input = targets[:, :-1]\n",
    "            decoder_target = targets[:, 1:]\n",
    "            # Create the target mask\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through the model\n",
    "            with autocast():\n",
    "                logits = model(keypoints, emotions, decoder_input, tgt_mask=tgt_mask)\n",
    "                # Compute the loss\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), decoder_target.reshape(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct = ((preds == decoder_target) & (decoder_target != pad_token_id) & (decoder_target != eos_token_id)).sum().item()\n",
    "            total = (decoder_target != pad_token_id).sum().item()\n",
    "\n",
    "            # Update the training values\n",
    "            train_correct += correct\n",
    "            train_total += total\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Initialize the values for validation\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            # Iterate through the validation data\n",
    "            for keypoints, emotions, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "                # Move the data to the device\n",
    "                keypoints, emotions, targets = keypoints.to(device), emotions.to(device), targets.to(device)\n",
    "                # Create the decoder input and target sequences\n",
    "                decoder_input = targets[:, :-1]\n",
    "                decoder_target = targets[:, 1:]\n",
    "                # Create the target mask\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    # Forward pass through the model\n",
    "                    logits = model(keypoints, emotions, decoder_input, tgt_mask=tgt_mask)\n",
    "                    # Compute the loss\n",
    "                    loss = criterion(logits.view(-1, logits.size(-1)), decoder_target.reshape(-1))\n",
    "\n",
    "                # Compute the accuracy\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct = ((preds == decoder_target) & (decoder_target != pad_token_id)).sum().item()\n",
    "                total = (decoder_target != pad_token_id).sum().item()\n",
    "\n",
    "                # Update the validation values\n",
    "                val_correct += correct\n",
    "                val_total += total\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Compute the average loss and accuracy\n",
    "        train_acc = train_correct / train_total * 100\n",
    "        val_acc = val_correct / val_total * 100\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Append the losses and accuracies to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_losses.append(val_loss)\n",
    "        test_accuracies.append(val_acc)\n",
    "\n",
    "        # Print the summary for the epoch\n",
    "        print(f\"Epoch {epoch+1} Summary:\\nTrain Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Print the sample decoded text\n",
    "        sample_kps, sample_emo, _ = next(iter(val_loader))\n",
    "        print(\"Sample decoded:\", sample_decode_beam(model, sample_kps[0].to(device), sample_emo[0].to(device), tokenizer))\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            # Save the best model\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_emotion_model_new.pt\")\n",
    "        else:\n",
    "            # Increment the patience counter\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. {patience_counter} / {patience}\")\n",
    "            # Check if patience is exceeded\n",
    "            if patience_counter >= patience:\n",
    "                # Stop training\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Graph the training and validation losses and accuracies\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, test_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, test_losses, label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8342e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion for loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0)\n",
    "\n",
    "# Define the optimizer for the model\n",
    "optimizer = torch.optim.AdamW(emotion_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "total_steps = len(emotion_train_loader) * 500\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps * 0.05),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache and collect garbage\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19487188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model_emotion(emotion_model, emotion_train_loader, emotions_test_loader, optimizer, criterion, scheduler, device, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f404e",
   "metadata": {},
   "source": [
    "Example output:\n",
    "Epoch 24/500 [Train]: 100%|██████████| 36/36 [00:02<00:00, 15.41it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 37.90it/s]\n",
    "Epoch 24 Summary:\n",
    "Train Loss: 5.1715, Acc: 14.17% | Val Loss: 5.7470, Acc: 20.38%\n",
    "Sample decoded: it ' s.\n",
    "Epoch 25/500 [Train]: 100%|██████████| 36/36 [00:02<00:00, 15.26it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 37.88it/s]\n",
    "Epoch 25 Summary:\n",
    "Train Loss: 5.1139, Acc: 14.86% | Val Loss: 5.7463, Acc: 21.83%\n",
    "Sample decoded: the ' s the ' s the ' s the ' s the ' s, i ' s, i ' s the ' s the ' s, i ' s the ' s the ' s the ' s, i ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s ' s ' s the ' s ' s ' s ' s ' s the ' s the ' s the ' s the ' s the ' s the ' s ' s ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the, the, i ' s the ' s the ' s the ' s the ' s the ' s the, i ' s the ' s the ' s i ' s the ' s the, i ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the ' s the,\n",
    "Epoch 26/500 [Train]: 100%|██████████| 36/36 [00:02<00:00, 15.38it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 39.33it/s]\n",
    "Epoch 26 Summary:\n",
    "Train Loss: 5.0595, Acc: 15.62% | Val Loss: 5.7328, Acc: 22.00%\n",
    "Sample decoded: it ' s the ' s the ' s the ' s the ' s the.\n",
    "Epoch 27/500 [Train]: 100%|██████████| 36/36 [00:02<00:00, 15.70it/s]\n",
    "Validating: 100%|██████████| 8/8 [00:00<00:00, 37.97it/s]\n",
    "Epoch 27 Summary:\n",
    "Train Loss: 5.0064, Acc: 16.05% | Val Loss: 5.6538, Acc: 21.63%\n",
    "Sample decoded: it ' s."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
