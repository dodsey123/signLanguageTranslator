{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d353832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vtt(vtt_path, video_id):\n",
    "    def time_str_to_seconds(time_str):\n",
    "        h, m, s = time_str.split(\":\")\n",
    "        s, ms = s.split(\".\")\n",
    "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "    with open(vtt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vtt_text = f.read()\n",
    "\n",
    "    blocks = re.split(r'\\n\\n+', vtt_text.strip())\n",
    "    entries = []\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().splitlines()\n",
    "        if len(lines) >= 2 and \"-->\" in lines[0]:\n",
    "            start, end = lines[0].split(\" --> \")\n",
    "            text = \" \".join(lines[1:]).strip()\n",
    "            entries.append({\n",
    "                \"start\": time_str_to_seconds(start.strip()),\n",
    "                \"end\": time_str_to_seconds(end.strip()),\n",
    "                \"text\": text,\n",
    "                \"video_id\": video_id  # To distinguish overlapping times\n",
    "            })\n",
    "\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3614f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(path):\n",
    "    return os.path.basename(path).split(\"_keypoints\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keypoints_path = \"/content/drive/MyDrive/model_training/normalised_keypoints_output_extracted/batch1/train\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"/content/drive/MyDrive/model_training/subtitles/batch1/train\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "keypoints = []\n",
    "subtitles = []\n",
    "\n",
    "for k in keypoints_files:\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  print(k)\n",
    "  temp_keypoints = torch.load(k)\n",
    "  keypoints.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "counter  = 0\n",
    "\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    subtitles.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/model_training/i3d_features/train\"\n",
    "clip_gt_files = glob(os.path.join(path, \"*_clip_gt_tensor.pt\"))\n",
    "clip_ix_files = glob(os.path.join(path, \"*_clip_ix_tensor.pt\"))\n",
    "clip_preds_files = glob(os.path.join(path, \"*_preds_tensor.pt\"))\n",
    "\n",
    "clip_gt_features = []\n",
    "clip_ix_features = []\n",
    "clip_preds_features = []\n",
    "\n",
    "for gt in clip_gt_files:\n",
    "  base_name = os.path.basename(gt).replace(\"_clip_gt_tensor.pt\", \"\")\n",
    "  print(gt)\n",
    "  temp_gt = torch.load(gt)\n",
    "  clip_gt_features.append((base_name, temp_gt))\n",
    "\n",
    "for ix in clip_ix_files:\n",
    "  base_name = os.path.basename(ix).replace(\"_clip_ix_tensor.pt\", \"\")\n",
    "  print(ix)\n",
    "  temp_ix = torch.load(ix)\n",
    "  clip_ix_features.append((base_name, temp_ix))\n",
    "\n",
    "for pred in clip_preds_files:\n",
    "  base_name = os.path.basename(pred).replace(\"_preds_tensor.pt\", \"\")\n",
    "  print(pred)\n",
    "  temp_pred = torch.load(pred)\n",
    "  clip_preds_features.append((base_name, temp_pred))\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swin_features(path):\n",
    "    features_np = np.load(path)\n",
    "    print(f\"Loaded NumPy array with shape {features_np.shape} and dtype {features_np.dtype}\")\n",
    "\n",
    "    features_tensor = torch.from_numpy(features_np)\n",
    "    features_tensor = features_tensor.float().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee941ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/model_training/swin_features/train\"\n",
    "swin_files = glob(os.path.join(path, \"*.npy\"))\n",
    "\n",
    "swin_features = []\n",
    "\n",
    "for sw in swin_files:\n",
    "  base_name = os.path.basename(sw).replace(\".npy\", \"\")\n",
    "  print(sw)\n",
    "  temp_sw = get_swin_features(sw)\n",
    "  swin_features.append((base_name, temp_sw))\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfced62",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_path = \"/content/drive/MyDrive/model_training/normalised_keypoints_output_extracted/batch1/test\"\n",
    "keypoints_files = glob(os.path.join(keypoints_path, \"*_keypoints.pth\"))\n",
    "subtitles_path = \"/content/drive/MyDrive/model_training/subtitles/batch1/test\"\n",
    "subtitles_files = glob(os.path.join(subtitles_path, \"*.vtt\"))\n",
    "\n",
    "print(len(glob(keypoints_path)))\n",
    "\n",
    "keypoints_test = []\n",
    "subtitles_test = []\n",
    "\n",
    "for k in keypoints_files:\n",
    "  print(k)\n",
    "  base_name = os.path.basename(k).replace(\"_keypoints.pth\", \"\")\n",
    "  temp_keypoints = torch.load(k)\n",
    "  keypoints_test.append((extract_video_id(k), temp_keypoints))\n",
    "\n",
    "for s in subtitles_files:\n",
    "    print(s)\n",
    "    base_name = os.path.basename(s).replace(\".vtt\", \"\")\n",
    "    parsed_subs = parse_vtt(s, base_name)\n",
    "    subtitles_test.extend(parsed_subs)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c407e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/model_training/i3d_features/test\"\n",
    "clip_gt_files = glob(os.path.join(path, \"*_clip_gt_tensor.pt\"))\n",
    "clip_ix_files = glob(os.path.join(path, \"*_clip_ix_tensor.pt\"))\n",
    "clip_preds_files = glob(os.path.join(path, \"*_preds_tensor.pt\"))\n",
    "\n",
    "clip_gt_features_test = []\n",
    "clip_ix_features_test = []\n",
    "clip_preds_features_test = []\n",
    "\n",
    "for gt in clip_gt_files:\n",
    "  base_name = os.path.basename(gt).replace(\"_clip_gt_tensor.pt\", \"\")\n",
    "  print(gt)\n",
    "  temp_gt = torch.load(gt)\n",
    "  clip_gt_features_test.append((base_name, temp_gt))\n",
    "\n",
    "for ix in clip_ix_files:\n",
    "  base_name = os.path.basename(ix).replace(\"_clip_ix_tensor.pt\", \"\")\n",
    "  print(ix)\n",
    "  temp_ix = torch.load(ix)\n",
    "  clip_ix_features_test.append((base_name, temp_ix))\n",
    "\n",
    "for pred in clip_preds_files:\n",
    "  base_name = os.path.basename(pred).replace(\"_preds_tensor.pt\", \"\")\n",
    "  print(pred)\n",
    "  temp_pred = torch.load(pred)\n",
    "  clip_preds_features_test.append((base_name, temp_pred))\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/model_training/swin_features/test\"\n",
    "swin_files = glob(os.path.join(path, \"*.npy\"))\n",
    "\n",
    "swin_features_test = []\n",
    "\n",
    "for sw in swin_files:\n",
    "  base_name = os.path.basename(sw).replace(\".npy\", \"\")\n",
    "  print(sw)\n",
    "  temp_sw = get_swin_features(sw)\n",
    "  swin_features_test.append((base_name, temp_sw))\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1782b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = torch.cat([feats for _, feats in swin_features_test], dim=0) \n",
    "\n",
    "mean = all_feats.mean(dim=0)  # [D_sw]\n",
    "std  = all_feats.std(dim=0)   # [D_sw]\n",
    "eps  = 1e-6\n",
    "\n",
    "normalized_swin_features_test = []\n",
    "for vid, feats in swin_features_test:\n",
    "    norm_feats = (feats - mean) / (std + eps)\n",
    "    normalized_swin_features_test.append((vid, norm_feats))\n",
    "\n",
    "normalized_swin_dict_test = dict(normalized_swin_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_float(time_str):\n",
    "\n",
    "    # Split the time string into hours, minutes, and seconds\n",
    "    hours, minutes, seconds = time_str.split(':')\n",
    "\n",
    "    # Convert to float\n",
    "    hours = float(hours)\n",
    "    minutes = float(minutes)\n",
    "    seconds = float(seconds)\n",
    "\n",
    "    # Convert to total seconds\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class AllFeaturesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keypoints_data: dict,\n",
    "        subtitle_entries: list,\n",
    "        clip_gt_data: dict,\n",
    "        clip_ix_data: dict,\n",
    "        clip_preds_data: dict,\n",
    "        swin_data: dict,\n",
    "        fps: int = 25,\n",
    "        tokenizer=None,\n",
    "        max_length: int = 80,\n",
    "        num_joints: int = 25,\n",
    "    ):\n",
    "        self.fps = fps\n",
    "        self.max_length = max_length\n",
    "        self.num_joints = num_joints\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # ensure fast lookup\n",
    "        self.keypoints_data   = keypoints_data\n",
    "        self.clip_gt_data     = clip_gt_data\n",
    "        self.clip_ix_data     = clip_ix_data\n",
    "        self.clip_preds_data  = clip_preds_data\n",
    "        self.swin_data        = swin_data\n",
    "\n",
    "        self.samples = self.build_samples(subtitle_entries)\n",
    "\n",
    "    def build_samples(self, subtitles):\n",
    "        samples = []\n",
    "\n",
    "        for sub in subtitles:\n",
    "            vid = sub[\"video_id\"]\n",
    "            if not (\n",
    "                vid in self.keypoints_data\n",
    "                and vid in self.clip_gt_data\n",
    "                and vid in self.clip_ix_data\n",
    "                and vid in self.clip_preds_data\n",
    "                and vid in self.swin_data\n",
    "            ):\n",
    "                continue\n",
    "            kp_seq  = self.keypoints_data[vid]    # list of N_frame frames\n",
    "            gt_feat = self.clip_gt_data[vid]      # [N_clip × D_gt]\n",
    "            ix_feat = self.clip_ix_data[vid]      # [N_clip × D_ix]\n",
    "            pr_feat = self.clip_preds_data[vid]   # [N_clip × D_pr]\n",
    "            sw_feat = self.swin_data[vid]         # [N_frame × D_sw]\n",
    "\n",
    "            N_frame = len(kp_seq)\n",
    "            N_clip  = gt_feat.size(0)\n",
    "\n",
    "            start_f = int(sub[\"start\"] * self.fps)\n",
    "            end_f   = int(sub[\"end\"]   * self.fps)\n",
    "            if end_f <= start_f or end_f > N_frame:\n",
    "                continue\n",
    "\n",
    "            T = end_f - start_f\n",
    "            frame_idxs = np.arange(start_f, end_f) \n",
    "\n",
    "            clip_idxs = np.floor(frame_idxs * (N_clip / N_frame)).astype(int)\n",
    "            clip_idxs = np.clip(clip_idxs, 0, N_clip - 1) \n",
    "\n",
    "            gt_tensor = gt_feat[clip_idxs].float().unsqueeze(-1).to(self.device)\n",
    "            ix_tensor = ix_feat[clip_idxs].float().unsqueeze(-1).to(self.device)\n",
    "            pr_tensor = pr_feat[clip_idxs].float().to(self.device)\n",
    "\n",
    "            processed_kps = []\n",
    "            for fi in frame_idxs:\n",
    "                frame = kp_seq[fi]\n",
    "                if frame and len(frame[0]) > 0:\n",
    "                    person = frame[0]\n",
    "                    flat = [c for part in person for joint in part for c in joint]\n",
    "                else:\n",
    "                    flat = []\n",
    "                flat = flat[: self.num_joints * 3] \\\n",
    "                       + [0] * max(0, self.num_joints * 3 - len(flat))\n",
    "                processed_kps.append(torch.tensor(flat, dtype=torch.float32))\n",
    "            kp_tensor = torch.stack(processed_kps).to(self.device) \n",
    "\n",
    "            sw_tensor = sw_feat[frame_idxs].to(self.device)\n",
    "\n",
    "            tok = self.tokenizer(\n",
    "                sub[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].squeeze(0).to(self.device)\n",
    "\n",
    "            samples.append({\n",
    "                \"keypoints\":  kp_tensor,\n",
    "                \"clip_gt\":    gt_tensor,\n",
    "                \"clip_ix\":    ix_tensor,\n",
    "                \"clip_preds\": pr_tensor,\n",
    "                \"swin\":       sw_tensor,\n",
    "                \"tokens\":     tok,\n",
    "            })\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "keypoints_dict   = dict(keypoints) \n",
    "clip_gt_dict     = dict(clip_gt_features) \n",
    "clip_ix_dict     = dict(clip_ix_features)  \n",
    "clip_preds_dict  = dict(clip_preds_features)\n",
    "swin_dict        = dict(swin_features)\n",
    "\n",
    "all_features_train_dataset = AllFeaturesDataset(keypoints_data = keypoints_dict, subtitle_entries = subtitles, clip_gt_data = clip_gt_dict, clip_ix_data = clip_ix_dict, clip_preds_data = clip_preds_dict, swin_data = normalized_swin_dict, fps = 25, tokenizer = tokenizer, max_length = 80, num_joints = 25)\n",
    "\n",
    "print(\"Dataset size:\", len(all_features_train_dataset))\n",
    "\n",
    "sample = all_features_train_dataset[0]\n",
    "kp_tensor    = sample[\"keypoints\"]   # [T x (25*3)]\n",
    "clip_gt      = sample[\"clip_gt\"]     # [T x D_gt]\n",
    "clip_ix      = sample[\"clip_ix\"]     # [T x D_ix]\n",
    "clip_preds   = sample[\"clip_preds\"]  # [T x D_pr]\n",
    "swin_feats   = sample[\"swin\"]        # [T x D_sw]\n",
    "tokens       = sample[\"tokens\"]      # [max_length]\n",
    "\n",
    "print(\"Keypoints shape:   \", kp_tensor.shape)\n",
    "print(\"CLIP GT shape:     \", clip_gt.shape)\n",
    "print(\"CLIP IX shape:     \", clip_ix.shape)\n",
    "print(\"CLIP preds shape:  \", clip_preds.shape)\n",
    "print(\"Swin features shape:\", swin_feats.shape)\n",
    "print(\"Token IDs shape:   \", tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "keypoints_test_dict   = dict(keypoints_test)\n",
    "clip_gt_test_dict     = dict(clip_gt_features_test) \n",
    "clip_ix_test_dict     = dict(clip_ix_features_test) \n",
    "clip_preds_test_dict  = dict(clip_preds_features_test)\n",
    "swin_test_dict        = dict(swin_features_test)\n",
    "\n",
    "all_features_test_dataset = AllFeaturesDataset(keypoints_data = keypoints_test_dict, subtitle_entries = subtitles_test, clip_gt_data = clip_gt_test_dict, clip_ix_data = clip_ix_test_dict, clip_preds_data = clip_preds_test_dict, swin_data = normalized_swin_dict_test, fps = 25, tokenizer = tokenizer, max_length = 80, num_joints = 25)\n",
    "\n",
    "print(\"Dataset size:\", len(all_features_test_dataset))\n",
    "\n",
    "sample = all_features_train_dataset[0]\n",
    "kp_tensor    = sample[\"keypoints\"]   # [T x (25*3)]\n",
    "clip_gt      = sample[\"clip_gt\"]     # [T x D_gt]\n",
    "clip_ix      = sample[\"clip_ix\"]     # [T x D_ix]\n",
    "clip_preds   = sample[\"clip_preds\"]  # [T x D_pr]\n",
    "swin_feats   = sample[\"swin\"]        # [T x D_sw]\n",
    "tokens       = sample[\"tokens\"]      # [max_length]\n",
    "\n",
    "print(\"Keypoints shape:   \", kp_tensor.shape)\n",
    "print(\"CLIP GT shape:     \", clip_gt.shape)\n",
    "print(\"CLIP IX shape:     \", clip_ix.shape)\n",
    "print(\"CLIP preds shape:  \", clip_preds.shape)\n",
    "print(\"Swin features shape:\", swin_feats.shape)\n",
    "print(\"Token IDs shape:   \", tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8caed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def all_features_collate_fn(batch):\n",
    "    keypoints =     [item[\"keypoints\"]   for item in batch]  # list of [T_i × (J*3)]\n",
    "    clip_gt =       [item[\"clip_gt\"]     for item in batch]  # list of [T_i × D_gt]\n",
    "    clip_ix =       [item[\"clip_ix\"]     for item in batch]  # list of [T_i × D_ix]\n",
    "    clip_preds =    [item[\"clip_preds\"]  for item in batch]  # list of [T_i × D_pr]\n",
    "    swin =          [item[\"swin\"]        for item in batch]  # list of [T_i × D_sw]\n",
    "    tokens =        [item[\"tokens\"]      for item in batch]  # list of [L]\n",
    "\n",
    "    keypoints_padded  = pad_sequence(keypoints,  batch_first=True, padding_value=0.0)\n",
    "    clip_gt_padded    = pad_sequence(clip_gt,    batch_first=True, padding_value=0.0)\n",
    "    clip_ix_padded    = pad_sequence(clip_ix,    batch_first=True, padding_value=0.0)\n",
    "    clip_preds_padded = pad_sequence(clip_preds, batch_first=True, padding_value=0.0)\n",
    "    swin_padded       = pad_sequence(swin,       batch_first=True, padding_value=0.0)\n",
    "\n",
    "    tokens_stacked    = torch.stack(tokens, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"keypoints\": keypoints_padded,   # FloatTensor[B × T_max × (J*3)]\n",
    "        \"clip_gt\": clip_gt_padded,       # FloatTensor[B × T_max × D_gt]\n",
    "        \"clip_ix\": clip_ix_padded,       # FloatTensor[B × T_max × D_ix]\n",
    "        \"clip_preds\": clip_preds_padded, # FloatTensor[B × T_max × D_pr]\n",
    "        \"swin\": swin_padded,             # FloatTensor[B × T_max × D_sw]\n",
    "        \"tokens\": tokens_stacked,        # LongTensor [B × L]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_train_loader = DataLoader(all_features_train_dataset, batch_size=32, shuffle=True, collate_fn=all_features_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_test_loader = DataLoader(all_features_test_dataset, batch_size=32, shuffle=True, collate_fn=all_features_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, d_model]\n",
    "        T = x.size(1)\n",
    "        if T > self.pe.size(1):\n",
    "            raise ValueError(f\"Sequence length {T} exceeds {self.pe.size(1)}\")\n",
    "        return x + self.pe[:, :T, :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllFeaturesEncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keypoints_dim: int = 75,\n",
    "        clip_gt_dim:    int = 1,\n",
    "        clip_ix_dim:    int = 1,\n",
    "        clip_preds_dim: int = 1024,\n",
    "        swin_dim:       int = 768,\n",
    "        d_model:        int = 384,\n",
    "        num_heads:      int = 6,\n",
    "        num_layers:     int = 2,\n",
    "        cross_layers:   int = 2, \n",
    "        ff_dim:         int = 512,\n",
    "        max_len:        int = 1024,\n",
    "        vocab_size:     int = 30522,\n",
    "        pad_idx:        int = 0,\n",
    "        dropout:        float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kp_proj = nn.Linear(keypoints_dim, d_model)\n",
    "        self.gt_proj = nn.Linear(clip_gt_dim,    d_model)\n",
    "        self.ix_proj = nn.Linear(clip_ix_dim,    d_model)\n",
    "        self.pr_proj = nn.Linear(clip_preds_dim, d_model)\n",
    "        self.sw_proj = nn.Linear(swin_dim,       d_model)\n",
    "\n",
    "        self.pe       = PositionalEncoding(d_model, max_len)\n",
    "        self.input_dp = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer   = nn.TransformerEncoderLayer(d_model, num_heads, ff_dim, dropout=dropout)\n",
    "        self.kp_enc = nn.TransformerEncoder(enc_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "        self.gt_enc = nn.TransformerEncoder(enc_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "        self.ix_enc = nn.TransformerEncoder(enc_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "        self.pr_enc = nn.TransformerEncoder(enc_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "        self.sw_enc = nn.TransformerEncoder(enc_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "\n",
    "        self.cross_attn   = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.cross_ln     = nn.LayerNorm(d_model)\n",
    "        self.cross_dp     = nn.Dropout(dropout)\n",
    "        self.cross_layers = cross_layers\n",
    "\n",
    "        dec_layer    = nn.TransformerDecoderLayer(d_model, num_heads, ff_dim, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers, norm=nn.LayerNorm(d_model))\n",
    "\n",
    "        self.embed  = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.out_fc = nn.Linear(d_model, vocab_size)\n",
    "        self.out_fc.weight = self.embed.weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        keypoints,    # [B, T, keypoints_dim]\n",
    "        clip_gt,      # [B, T, 1]\n",
    "        clip_ix,      # [B, T, 1]\n",
    "        clip_preds,   # [B, T, 1024]\n",
    "        swin,         # [B, T, 768]\n",
    "        input_ids,    # [B, L]\n",
    "        tgt_mask=None # [L, L] causal mask\n",
    "    ):\n",
    "        B, T, _ = keypoints.shape\n",
    "\n",
    "        src_pad = (keypoints.abs().sum(-1) == 0)  # [B, T]\n",
    "\n",
    "        kp = self.input_dp(self.pe(self.kp_proj(keypoints)))\n",
    "        gt = self.input_dp(self.pe(self.gt_proj(clip_gt)))\n",
    "        ix = self.input_dp(self.pe(self.ix_proj(clip_ix)))\n",
    "        pr = self.input_dp(self.pe(self.pr_proj(clip_preds)))\n",
    "        sw = self.input_dp(self.pe(self.sw_proj(swin)))\n",
    "\n",
    "        kp = kp.permute(1,0,2)\n",
    "        gt = gt.permute(1,0,2)\n",
    "        ix = ix.permute(1,0,2)\n",
    "        pr = pr.permute(1,0,2)\n",
    "        sw = sw.permute(1,0,2)\n",
    "\n",
    "        kp_mem = self.kp_enc(kp, src_key_padding_mask=src_pad)\n",
    "        gt_mem = self.gt_enc(gt, src_key_padding_mask=src_pad)\n",
    "        ix_mem = self.ix_enc(ix, src_key_padding_mask=src_pad)\n",
    "        pr_mem = self.pr_enc(pr, src_key_padding_mask=src_pad)\n",
    "        sw_mem = self.sw_enc(sw, src_key_padding_mask=src_pad)\n",
    "\n",
    "        kv     = torch.cat([gt_mem, ix_mem, pr_mem, sw_mem], dim=0)\n",
    "\n",
    "        kv_pad = src_pad.repeat(1, 4)\n",
    "\n",
    "        q = kp_mem \n",
    "        for _ in range(self.cross_layers):\n",
    "            attn_out, _ = self.cross_attn(\n",
    "\n",
    "                query            = q,\n",
    "                key              = kv,\n",
    "                value            = kv,\n",
    "                attn_mask        = None,  \n",
    "                key_padding_mask = kv_pad,\n",
    "                need_weights     = False\n",
    "            )\n",
    "            q = self.cross_ln(q + self.cross_dp(attn_out))\n",
    "\n",
    "        memory = q \n",
    "\n",
    "        tgt     = self.embed(input_ids)               # [B, L, d_model]\n",
    "        tgt     = self.input_dp(self.pe(tgt)).permute(1,0,2)  # → [L, B, d_model]\n",
    "        tgt_pad = (input_ids == self.embed.padding_idx)      # [B, L]\n",
    "\n",
    "        out = self.decoder(\n",
    "            tgt,        # [L, B, d_model]\n",
    "            memory,     # [T, B, d_model]\n",
    "            tgt_mask,   # [L, L] causal mask for self-attn\n",
    "            None,       # no 2D mask on cross-attn\n",
    "            tgt_pad,    # [B, L]\n",
    "            src_pad     # [B, T]\n",
    "        )\n",
    "\n",
    "        logits = self.out_fc(out).permute(1,0,2)  # → [B, L, V]\n",
    "\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424098c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_model = AllFeaturesEncoderDecoderTransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50088bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_decode_beam(\n",
    "    model,\n",
    "    keypoints,     # [T, 75]\n",
    "    clip_gt,       # [T, 1]\n",
    "    clip_ix,       # [T, 1]\n",
    "    clip_preds,    # [T, 1024]\n",
    "    swin,          # [T, 768]\n",
    "    tokenizer,\n",
    "    beam_width=3,\n",
    "    max_len=80,\n",
    "    eos_token_id=102,\n",
    "):\n",
    "    model.eval()\n",
    "    device = keypoints.device\n",
    "\n",
    "    generated = [(torch.tensor([tokenizer.cls_token_id], device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in generated:\n",
    "            if seq[-1].item() == eos_token_id:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            input_ids = seq.unsqueeze(0)  # [1, L]\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                input_ids.size(1)\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(\n",
    "                    keypoints.unsqueeze(0),    # [1, T, 75]\n",
    "                    clip_gt.unsqueeze(0),      # [1, T, 1]\n",
    "                    clip_ix.unsqueeze(0),      # [1, T, 1]\n",
    "                    clip_preds.unsqueeze(0),   # [1, T, 1024]\n",
    "                    swin.unsqueeze(0),         # [1, T, 768]\n",
    "                    input_ids,                 # [1, L]\n",
    "                    tgt_mask=tgt_mask\n",
    "                )  # → [1, L, V]\n",
    "\n",
    "                next_logits = logits[0, -1, :]       # [V]\n",
    "                probs = torch.softmax(next_logits, dim=-1)\n",
    "                top_probs, top_idx = probs.topk(beam_width)\n",
    "\n",
    "                for p, idx in zip(top_probs, top_idx):\n",
    "                    new_seq = torch.cat([seq, idx.unsqueeze(0)])\n",
    "                    new_score = score - torch.log(p + 1e-12)\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        generated = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "\n",
    "    return tokenizer.decode(generated[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e293462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    device,\n",
    "    tokenizer,\n",
    "    num_epochs=300,\n",
    "    eos_token_id=102,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience, patience_counter = 15, 0\n",
    "\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        tot_loss = tot_correct = tot_tokens = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            kp      = batch[\"keypoints\"].to(device)    # [B×T×75]\n",
    "            gt      = batch[\"clip_gt\"].to(device)      # [B×T×D_gt]\n",
    "            ix      = batch[\"clip_ix\"].to(device)      # [B×T×D_ix]\n",
    "            pr      = batch[\"clip_preds\"].to(device)   # [B×T×D_pr]\n",
    "            sw      = batch[\"swin\"].to(device)         # [B×T×D_sw]\n",
    "            targets = batch[\"tokens\"].to(device)       # [B×L]\n",
    "\n",
    "            if kp.size(1) > 1024 or targets.size(1) > 80:\n",
    "                continue\n",
    "\n",
    "            decoder_input  = targets[:, :-1]\n",
    "            decoder_target = targets[:, 1:]\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                decoder_input.size(1)\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(kp, gt, ix, pr, sw, decoder_input, tgt_mask=tgt_mask)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                decoder_target.reshape(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(-1)\n",
    "            mask = (decoder_target != pad_token_id) & (decoder_target != eos_token_id)\n",
    "            correct = ((preds == decoder_target) & mask).sum().item()\n",
    "            total   = mask.sum().item()\n",
    "\n",
    "            tot_correct += correct\n",
    "            tot_tokens  += total\n",
    "            tot_loss    += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        train_acc = tot_correct / tot_tokens * 100\n",
    "        train_loss = tot_loss / len(train_loader)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = val_correct = val_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                kp      = batch[\"keypoints\"].to(device)\n",
    "                gt      = batch[\"clip_gt\"].to(device)\n",
    "                ix      = batch[\"clip_ix\"].to(device)\n",
    "                pr      = batch[\"clip_preds\"].to(device)\n",
    "                sw      = batch[\"swin\"].to(device)\n",
    "                targets = batch[\"tokens\"].to(device)\n",
    "\n",
    "                if kp.size(1) > 1024 or targets.size(1) > 80:\n",
    "                    continue\n",
    "\n",
    "                decoder_input  = targets[:, :-1]\n",
    "                decoder_target = targets[:, 1:]\n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                    decoder_input.size(1)\n",
    "                ).to(device)\n",
    "\n",
    "                logits = model(kp, gt, ix, pr, sw, decoder_input, tgt_mask=tgt_mask)\n",
    "                loss = criterion(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    decoder_target.reshape(-1)\n",
    "                )\n",
    "\n",
    "                preds = logits.argmax(-1)\n",
    "                mask = decoder_target != pad_token_id\n",
    "                correct = ((preds == decoder_target) & mask).sum().item()\n",
    "                total = mask.sum().item()\n",
    "\n",
    "                val_correct += correct\n",
    "                val_tokens  += total\n",
    "                val_loss    += loss.item()\n",
    "\n",
    "        val_acc = val_correct / val_tokens * 100\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        for batch in val_loader:\n",
    "            kp_s, gt_s, ix_s, pr_s, sw_s = (\n",
    "                batch[\"keypoints\"][0],\n",
    "                batch[\"clip_gt\"][0],\n",
    "                batch[\"clip_ix\"][0],\n",
    "                batch[\"clip_preds\"][0],\n",
    "                batch[\"swin\"][0],\n",
    "            )\n",
    "            sample_text = sample_decode_beam(\n",
    "                model,\n",
    "                kp_s.to(device),\n",
    "                gt_s.to(device),\n",
    "                ix_s.to(device),\n",
    "                pr_s.to(device),\n",
    "                sw_s.to(device),\n",
    "                tokenizer,\n",
    "                beam_width=3,\n",
    "                max_len=80,\n",
    "                eos_token_id=tokenizer.sep_token_id \n",
    "            )\n",
    "            print(\"Sample:\", sample_text)\n",
    "            break\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_all_features__model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy Over Epochs\"), plt.legend(), plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Over Epochs\"), plt.legend(), plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b110ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)  \n",
    "optimizer = torch.optim.AdamW(all_features_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(all_features_train_loader) * 500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_model(all_features_model, all_features_train_loader, all_features_test_loader, optimizer, criterion, scheduler, device, tokenizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9790f5d",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "\n",
    "Epoch 1 | Train Loss: 111.6891, Acc: 0.98% | Val   Loss: 47.6495, Acc: 2.07%\n",
    "Sample: \n",
    "Epoch 2/500 [Train]: 100%|██████████| 143/143 [00:15<00:00,  9.29it/s]\n",
    "Validating: 100%|██████████| 29/29 [00:01<00:00, 25.24it/s]\n",
    "Epoch 2 | Train Loss: 40.8139, Acc: 1.11% | Val   Loss: 32.9179, Acc: 2.56%\n",
    "Sample: \n",
    "Epoch 3/500 [Train]: 100%|██████████| 143/143 [00:15<00:00,  9.22it/s]\n",
    "Validating: 100%|██████████| 29/29 [00:01<00:00, 26.13it/s]\n",
    "Epoch 3 | Train Loss: 33.4913, Acc: 1.39% | Val   Loss: 28.9060, Acc: 3.62%\n",
    "Sample: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
    "Epoch 4/500 [Train]: 100%|██████████| 143/143 [00:15<00:00,  9.28it/s]\n",
    "Validating: 100%|██████████| 29/29 [00:01<00:00, 25.12it/s]\n",
    "Epoch 4 | Train Loss: 30.4782, Acc: 1.46% | Val   Loss: 27.2522, Acc: 7.35%\n",
    "Sample: ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
